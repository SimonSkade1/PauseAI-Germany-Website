export default function HintergrundSection() {
  return (
    <div className="appell-zitate-container">
      <span className="appell-accent-line"></span>
      <h2 className="appell-section-heading">Hintergrund</h2>

      <p className="appell-paragraph">
        KI steigert Produktivität und treibt medizinischen Fortschritt voran, doch die rasante Entwicklung
        birgt auch große Risiken, die über alle Ländergrenzen hinaus wirken. Warnungen weltweit
        führender Experten machen deutlich: Nur durch internationale Zusammenarbeit bekommt die
        Welt Gefahren von Missbrauch bis Kontrollverlust in den Griff.<sup>
          <a href="https://aistatement.com/" target="_blank" rel="noopener" className="appell-citation-link" title="International AI Statement">
            [1]
          </a>, {" "}
          <a href="https://superintelligence-statement.org/" target="_blank" rel="noopener" className="appell-citation-link" title="Superintelligence Statement">
            [2]
          </a>
        </sup>
      </p>

      <p className="appell-paragraph">
        Beim ersten AI Safety Summit (Bletchley 2023) warnte die <a href="https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023" target="_blank" rel="noopener" title="The Bletchley Declaration">Erklärung</a> vor dem „Potenzial für
        schwerwiegende, sogar katastrophale Schäden", und die Unterzeichner beschlossen,
        gemeinsam sichere KI zu gewährleisten.
      </p>

      <p className="appell-paragraph">
        Doch beim AI Action Summit (Paris 2025) verschwand Sicherheit hinter Wirtschaftsinteressen.
        <a href="https://time.com/7221384/ai-regulation-takes-backseat-paris-summit/" target="_blank" rel="noopener" title="TIME article on AI regulation at Paris Summit">Professor Max Tegmark vom MIT</a> sagte, „es fühlte sich fast so an, als würde man versuchen,
        Bletchley rückgängig zu machen," und nannte das Fehlen von Sicherheit in der <a href="https://fortune.com/2025/02/11/paris-ai-action-summit-ai-safety-sidelined-economic-opportunity-promoted/" target="_blank" rel="noopener" title="Fortune article on Paris AI Summit">Erklärung</a> „ein
        Rezept für eine Katastrophe."
      </p>

      <p className="appell-paragraph">
        Nun steht der AI Impact Summit (Delhi, 19.&ndash;20. Februar 2026) bevor, aber verbindliche
        Sicherheitsregeln stehen nicht auf der Agenda.
      </p>

      <p className="appell-paragraph">
        Von „Safety" zu „Action" zu „Impact": Sicherheit verschwindet aus dem Namen. Doch <a href="https://controlai.com/statement" target="_blank" rel="noopener" title="ControlAI statement on AI regulation">über 100
        britische Parlamentarier unterstützen verbindliche Regulierungen für die fortgeschrittensten
        KI-Systeme</a>, und <a href="https://red-lines.ai/" target="_blank" rel="noopener" title="Red Lines AI statement">zwölf Nobelpreisträger und hunderte Experten fordern</a>, dass „bis Ende 2026
        ein internationales Abkommen in Kraft treten soll, welches rote Linien für KI festlegt und deren
        Durchsetzung sicherstellt."
      </p>

      <p className="appell-paragraph">
        Der Gipfel in Delhi ist ein entscheidender Moment, diesen Aufrufen zu folgen und den Kurs zu
        ändern. Deutschland ist als gewichtige internationale Stimme gut positioniert, um dies
        anzustoßen.
      </p>
    </div>
  );
}
